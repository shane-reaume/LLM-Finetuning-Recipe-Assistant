# Configuration for TinyLlama fine-tuning on RecipeNLG dataset
# Optimized for NVIDIA GeForce RTX 5070 with full fine-tuning
# Memory-optimized settings for 8GB VRAM

# Data settings
data:
  dataset_name: "RecipeNLG"
  splits:
    train: "train"
    validation: "validation"
  prompt_template: "<|system|>You are a recipe assistant. Create detailed recipes with exact measurements and clear instructions.<|endoftext|>\n<|user|>Write a complete recipe using these ingredients: {ingredients}. Include a title, ingredients list with measurements, and numbered cooking steps.<|endoftext|>\n<|assistant|>"
  response_key: "instructions"
  max_length: 128  # Reduced from 128 to save memory
  max_train_samples: 500000  # Reduced from 25000 to save memory
  cache_dir: "./data/processed/generation"  # Added cache directory

# Model settings
model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  save_dir: "models/recipe_assistant_5070_geforce_rtx"
  load_in_8bit: false  # Disable 8-bit quantization
  gradient_checkpointing: true  # Enable gradient checkpointing
  use_cache: false  # Disable KV cache to save memory
  low_cpu_mem_usage: true  # Enable low CPU memory usage

# Training settings
training:
  batch_size: 32  # Reduced from 2 to save memory
  gradient_accumulation_steps: 32  # Increased from 128 to compensate for smaller batch size
  learning_rate: 2.0e-5
  weight_decay: 0.01
  num_train_epochs: 3
  warmup_steps: 100
  save_steps: 500
  save_total_limit: 2
  logging_dir: "logs/recipe_assistant_5070_geforce_rtx"
  evaluation_strategy: "steps"
  eval_steps: 500
  fp16: true  # Enable mixed precision training
  bf16: false  # Disable bfloat16
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  seed: 42
  # Optimizer settings
  optim_args: "eps=1e-8 betas=(0.9,0.999)"
  # Memory optimization settings
  dataloader_num_workers: 8  # Reduced from 2 to save memory
  dataloader_pin_memory: true  # Disable pin memory to save memory
  max_memory_MB: 7000  # Reduced from 9000 to leave more headroom
  lora:
    enabled: true
    r: 32
    alpha: 32
    dropout: 0.05
    target_modules: ["q_proj", "v_proj"]

# Testing settings
testing:
  num_beams: 4
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.2
  length_penalty: 1.0
  no_repeat_ngram_size: 3
  test_examples_file: "data/processed/recipe_test_examples.json"
  generation_params:
    max_new_tokens: 256  # Reduced to match training length
    temperature: 0.7
    top_p: 0.9
    top_k: 50
    do_sample: true
    repetition_penalty: 1.2
    no_repeat_ngram_size: 3
    remove_invalid_values: true
    num_beams: 1
  evaluation_metrics:
    - perplexity
    - rouge
  metrics:
    - "rouge"
    - "bertscore"
    - "recipe_quality" 